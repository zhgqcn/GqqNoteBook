---
title: 数据归一化
date: 2020-12-01 
categories: DeepLearning
id: Gqq
---

# 为什么要数据归一化`Feature Scaling`

1. 由于原始数据值的范围差异很大，因此在某些机器学习算法中，如果没有归一化，目标函数将无法正常工作。例如，许多分类器通过欧几里得距离来计算两点之间的距离。如果其中一个要素的取值范围较广，则该距离将受此特定要素支配。因此，所有特征的范围应归一化，以使每个特征对最终距离的贡献大致成比例

2. 可以使得梯度下降收敛更快

3. 如果将正则化用作损失函数的一部分，则数据归一化也很重要（以便适当地对系数进行惩罚）

   

# 归一化的优点

1. 提升模型的收敛速度

![s](https://tvax2.sinaimg.cn/large/005tpOh1ly1gl8fu5pj2xj30m8064wf8.jpg)

<center><font color='blue'>左图：归一化前 右图：归一化后</font></center>

2. 提高模型精度



# 常用方法

1. **均值方差归一化 Zero-mean normalization**

$$
x = \frac{x-\bar{x}}{\sigma }
$$

​		又叫**标准化**`Standardization`，处理后的数据满足标准正态分布

​		可见，<font color='red'>归一化包含标准化，标准化只是归一化的一种</font>

2. **Min-max normalization**

$$
x = \frac{x-x_{min}}{x_{max}-x_{min}}
$$

​		是一种线性的归一化方法，它的特点是不会对数据分布产生影响。

​		当数据集的分布最大值和最小值不稳定时，结果输出可能也会不稳定。

​		该方法在**图像处理**上很常用，因为大部分的图像像素值为`x∈[0, 255]`

3. **Non-linear normalization**

   包含`log`、`exp`、`arctan`、`sigmoid`等非线性函数，选择时取决于期望的输出范围：

   - `log()`在`[0,1]`上有很强的区分度
   - `arctan()`可以将任意实数转换到`[-π/2, π/2]`区间
   - `sigmoid()`可以将任意实数映射到`(0, 1)`区间

4. **Length-one normalization**

$$
x = \frac{x}{\left | \left | x \right | \right |}
$$

​		将特征转为单位向量的形式来剔除特征强度的影响

# 测试集归一化

测试集归一化所用均值和标准差的必须与`train dataset 的input`相同，保证数据是同分布的

- 测试集的数据分布本身是无法计算的，而深度学习神经网络的基础就是假定`test set`和`train set`是同分布的数据

# `target`归一化

1. `input`经过`target`下采样而来

- **利用target下采样得到input，此时，两者本身就是同分布的(如`img 1`），以至于在很多SR Task中，并没有采用normalize操作**

- 若要进行normalize，两者可采用相同的均值和标准差

![Snipaste_2020-12-01_23-29-30](https://tvax1.sinaimg.cn/large/005tpOh1ly1gl8rwkcf9rj30na064myt.jpg)

<center><font color='red'>img 1</font></center>

2. `input`和`target`都为`Raw images`

- **当input和target都是Raw images时，两者的数据分布将会存在较大的差异，如`img 2`中：**
  - 在R通道上，input和target的均值和标准差相差很大
  - 在G通道上，input和target的均值和标准差相差很小，接近于同分布

![Snipaste_2020-12-01_23-36-56](https://tvax2.sinaimg.cn/large/005tpOh1ly1gl8s4ayje7j30n406575x.jpg)

<center><font color='red'>img 2</font></center>

- <font color='blue'><b>此时，对于input和target应当采用不同的均值和标准差[即`img 2`计算的结果]，使得它们能够满足`(0,1)`正态分布</b></font>

  ```python
  def Dataset_transform_norm(imgs):
      if imgs == 'LR':
          norms = Normalize((0.1815, 0.0378, 0.0000), (0.1599, 0.0896, 1.0000)) # G通道无信息，所以标准差不能为0，因为分母不能为0
      elif imgs == 'HR_2':
          norms = Normalize((0.1290, 0.0367, 0.0000), (0.1053, 0.0830, 1.0000))
      else:
          norms = Normalize((0.1058, 0.0366, 0.0000), (0.0906, 0.0841, 1.0000))
      return Compose([
          ToTensor(),
          norms
      ])
  ```

  <font color='blue'><b>归一化后，满足正态分布，如`img 3`：</b></font>

  ![Snipaste_2020-12-01_23-48-30](https://tva2.sinaimg.cn/large/005tpOh1ly1gl8sgbh7rmj30mz0bpjun.jpg)

<center><font color='red'>img 3</font></center>

​		

# `pytorch`中的归一化

在`pytorch`中用**均值**和**标准差**对张量图像进行归一化

```python
torchvision.transforms.Normalize(mean, std, inplace=False)
```

所以，在数据预处理时有：

```python
transform_train = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])
```

## 计算均值与标准差

对于`RGB`图像，计算均值与标准差的代码👉[calculate_mean_std.py](https://github.com/zhgqcn/OpenSource/blob/main/tools/calculate_mean_std.py)：

**通过对General100数据集进行计算可知**

|            | **R通道** | **G通道** | **B通道** |
| ---------- | :-------: | :-------: | :-------: |
| **均值**   |  0.5525   |  0.4983   |  0.4047   |
| **标准差** |  0.2432   |  0.2403   |  0.2482   |

![Snipaste_2020-12-01_17-15-03](https://tvax3.sinaimg.cn/large/005tpOh1ly1gl8h2wvw3sj30mm02a3yy.jpg)

**之后，便可利用计算好的均值和标准差放入到`transforms.Normalize`中做归一化处理了**



> Write by `Gqq`